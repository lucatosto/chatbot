{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import nltk # For tokenize\n",
    "from tqdm import tqdm # Progress bar\n",
    "import pickle # Saving the data\n",
    "import os # Checking file existance\n",
    "\n",
    "from cornelldata import CornellData\n",
    "\n",
    "class TextData:\n",
    "    \"\"\"Dataset class\n",
    "    Warning: No vocabulary limit\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        \"\"\"Load all conversations\n",
    "        Args:\n",
    "            args: parametters of the model\n",
    "        \"\"\"\n",
    "        # Path variables\n",
    "        self.corpusDir = \"data/cornell/\"\n",
    "        self.samplesDir = \"data/samples/\"\n",
    "        self.samplesName = \"dataset.pkl\"\n",
    "        \n",
    "        self.goToken = -1 # Start of sequence\n",
    "        self.eosToken = -1 # End of sequence\n",
    "        self.unknownToken = -1 # Word dropped from vocabulary\n",
    "        \n",
    "        self.trainingSamples = [] # 2d array containing each question and his answer\n",
    "        \n",
    "        self.word2id = {}\n",
    "        self.id2word = {} # For a rapid conversion\n",
    "        \n",
    "        # Limits of the database (perfomances issues)\n",
    "        # Limit ??? self.maxExampleLen = options.maxExampleLen or 25\n",
    "        #self.linesLimit = #\n",
    "        \n",
    "        self.loadCorpus(self.samplesDir)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def loadCorpus(self, dirName):\n",
    "        \"\"\"Load/create the conversations data\n",
    "        Args:\n",
    "            dirName (str): The directory where to load/save the model\n",
    "        \"\"\"\n",
    "        datasetExist = False\n",
    "        if os.path.exists(dirName+self.samplesName):\n",
    "            datasetExist = True\n",
    "        \n",
    "        if not datasetExist: # Fist time we load the database: creating all files\n",
    "            print('Training samples not found. Creating dataset...')\n",
    "            # Corpus creation\n",
    "            cornellData = CornellData(self.corpusDir)\n",
    "            self.createCorpus(cornellData.getConversations())\n",
    "            \n",
    "            # Saving\n",
    "            print('Saving dataset...')\n",
    "            self.saveDataset(dirName) # Saving tf samples\n",
    "        else:\n",
    "            print('Loading dataset from %s...' % (dirName))\n",
    "            self.loadDataset(dirName)\n",
    "            \n",
    "            pass # TODO\n",
    "        \n",
    "        # TODO: Shuffle the dataset\n",
    "        \n",
    "        # Plot some stats:\n",
    "        print('Loaded: %d words, %d QA' % (len(self.word2id), len(self.trainingSamples)))\n",
    "    def saveDataset(self, dirName):\n",
    "        \"\"\"Save samples to file\n",
    "        Args:\n",
    "            dirName (str): The directory where to load/save the model\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(dirName + self.samplesName, 'wb') as handle:\n",
    "            data = {\n",
    "                \"word2id\": self.word2id,\n",
    "                \"id2word\": self.id2word,\n",
    "                \"trainingSamples\": self.trainingSamples\n",
    "                }\n",
    "            pickle.dump(data, handle, -1) # Using the highest protocol available\n",
    "\n",
    "    def loadDataset(self, dirName):\n",
    "        \"\"\"Load samples from file\n",
    "        Args:\n",
    "            dirName (str): The directory where to load the model\n",
    "        \"\"\"\n",
    "        with open(dirName + self.samplesName, 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            self.word2id = data[\"word2id\"]\n",
    "            self.id2word = data[\"id2word\"]\n",
    "            self.trainingSamples = data[\"trainingSamples\"]\n",
    "            \n",
    "            self.goToken = self.word2id[\"<go>\"]\n",
    "            self.eosToken = self.word2id [\"<eos>\"]\n",
    "            self.unknownToken = self.word2id[\"<unknown>\"] # Restore special words\n",
    "            \n",
    "            \n",
    "    \n",
    "    def createCorpus(self, conversations):\n",
    "        \"\"\"Extract all data from the given vocabulary\n",
    "        \"\"\"\n",
    "        # Add standard tokens\n",
    "        self.goToken = self.makeWordId(\"<go>\") # Start of sequence\n",
    "        self.eosToken = self.makeWordId(\"<eos>\") # End of sequence\n",
    "        self.unknownToken = self.makeWordId(\"<unknown>\") # Word dropped from vocabulary\n",
    "        \n",
    "        # Prepocessing data\n",
    "\n",
    "        for conversation in tqdm(conversations, desc=\"Extract conversations\"):\n",
    "            self.extractConversation(conversation)\n",
    "        \n",
    "        # TODO: Shuffling (before saving ?)\n",
    "        \n",
    "        # TODO: clear trainingSample after saving ?\n",
    "    def extractConversation(self, conversation):\n",
    "        \"\"\"Extract the sample lines from the conversations\n",
    "        Args:\n",
    "            conversation (Obj): a convesation object containing the lines to extract\n",
    "        \"\"\"\n",
    "            \n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in range(len(conversation[\"lines\"]) - 1): # We ignore the last line (no answer for it)\n",
    "            inputLine  = conversation[\"lines\"][i]\n",
    "            targetLine = conversation[\"lines\"][i+1]\n",
    "            \n",
    "            inputWords  = self.extractText(inputLine [\"text\"])\n",
    "            targetWords = self.extractText(targetLine[\"text\"], True)\n",
    "            \n",
    "            #print(inputLine)\n",
    "            #print(targetLine)\n",
    "            #print(inputWords)\n",
    "            #print(targetWords)\n",
    "            \n",
    "            if not inputWords or not targetWords: # If one of the list is empty\n",
    "                tqdm.write(\"Error with some sentences. Sample ignored.\")\n",
    "                if inputWords:\n",
    "                    tqdm.write(inputLine[\"text\"])\n",
    "                if targetWords:\n",
    "                    tqdm.write(targetLine[\"text\"])\n",
    "            else:\n",
    "                inputWords.reverse() # Reverse inputs (apparently not the output. Why ?)\n",
    "                \n",
    "                targetWords.insert(0, self.goToken)\n",
    "                targetWords.append(self.eosToken) # Add the end of string\n",
    "\n",
    "                self.trainingSamples.append([inputWords, targetWords])\n",
    "                #self.trainingSamples.append([tf.constant(inputWords), tf.constant(targetWords)]) # tf.cst ? or keep as array (to feed placeholder) ?\n",
    "        \n",
    "    \n",
    "    def extractText(self, line, isTarget=False):\n",
    "        \"\"\"Extract the words from a sample lines\n",
    "        Args:\n",
    "            line (str): a line containing the text to extract\n",
    "            isTarget (bool): Define the question on the answer\n",
    "        Return:\n",
    "            list<int>: the list of the word ids of the sentence\n",
    "        \"\"\"\n",
    "        words = []\n",
    "        \n",
    "        # TODO !!!\n",
    "        # If answer: we only keep the last sentence\n",
    "        # If question: we only keep the first sentence\n",
    "        \n",
    "        tokens = nltk.word_tokenize(line)\n",
    "        for token in tokens: # TODO: Limit size (if sentence too long) ?\n",
    "            words.append(self.makeWordId(token)) # Create the vocabulary and the training sentences\n",
    "        \n",
    "        return words\n",
    "    def makeWordId(self, word):\n",
    "        \"\"\"Add a word to the dictionary\n",
    "        Args:\n",
    "            word (str): word to add\n",
    "        Return:\n",
    "            int: the id of the word created\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get the id if the word already exist\n",
    "        id = self.word2id.get(word, -1)\n",
    "        \n",
    "        # If not, we create a new entry\n",
    "        if id == -1:\n",
    "            id = len(self.word2id)\n",
    "            self.word2id[word] = id\n",
    "            self.id2word[id] = word\n",
    "        \n",
    "        return id\n",
    "\n",
    "    def playADialog():\n",
    "        \"\"\"Print a random dialogue from the dataset\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
